{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP89nAHgJ+eaSSfCJY469O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KruthikaTS/Breast-Cancer-Detection/blob/main/breast_cancer_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMRn5GK53J0v"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd  #data processing\n",
        "import numpy as np   #performing linear algebra\n",
        "import matplotlib.pyplot as plt #visualising dataset/graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "print (data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPYAevF7-yW1",
        "outputId": "a94df345-a46f-49f0-ab1c-d8cb59522c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0    842302         M        17.99         10.38          122.80     1001.0   \n",
            "1    842517         M        20.57         17.77          132.90     1326.0   \n",
            "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
            "3  84348301         M        11.42         20.38           77.58      386.1   \n",
            "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
            "0  ...          17.33           184.60      2019.0            0.1622   \n",
            "1  ...          23.41           158.80      1956.0            0.1238   \n",
            "2  ...          25.53           152.50      1709.0            0.1444   \n",
            "3  ...          26.50            98.87       567.7            0.2098   \n",
            "4  ...          16.67           152.20      1575.0            0.1374   \n",
            "\n",
            "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
            "0             0.6656           0.7119                0.2654          0.4601   \n",
            "1             0.1866           0.2416                0.1860          0.2750   \n",
            "2             0.4245           0.4504                0.2430          0.3613   \n",
            "3             0.8663           0.6869                0.2575          0.6638   \n",
            "4             0.2050           0.4000                0.1625          0.2364   \n",
            "\n",
            "   fractal_dimension_worst  Unnamed: 32  \n",
            "0                  0.11890          NaN  \n",
            "1                  0.08902          NaN  \n",
            "2                  0.08758          NaN  \n",
            "3                  0.17300          NaN  \n",
            "4                  0.07678          NaN  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data cleaning and dropping unwanted columns\n",
        "data.drop([\"Unnamed: 32\", \"id\"], axis = 1, inplace = True)\n",
        "#change the values in diagnosis column from object to char"
      ],
      "metadata": {
        "id": "Pz0U8l8u--Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change the values in diagnosis column from object to char\n",
        "data.diagnosis = [1 if i=='M' else 0 for i in data.diagnosis]\n",
        "#data.diagnosis.map({'M':1, 'B':0})"
      ],
      "metadata": {
        "id": "odnpmw6vCa-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature and target selection\n",
        "x_data = np.array(data.drop(\"diagnosis\", axis = 1))\n",
        "y = np.array(data.diagnosis)"
      ],
      "metadata": {
        "id": "OPIcOwhICrQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalise (feature scaling)\n",
        "'''\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "'''\n",
        "x_data = (x_data - np.min(x_data))/np.max(x_data) - np.min(x_data)"
      ],
      "metadata": {
        "id": "F_Apk5jhDxBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalisation ensures all feature values are between 0 & 1 â†’ makes ML model work better.\n"
      ],
      "metadata": {
        "id": "3Q4nedtsEsjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data to test and training\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 42)"
      ],
      "metadata": {
        "id": "AoIhkZMgE-VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test size = 15%, training size = 85%"
      ],
      "metadata": {
        "id": "FbxuyEu5Fek_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transpose\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size = 0.15, random_state = 42)\n",
        "\n",
        "x_train = x_train.T\n",
        "x_test = x_test.T\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n",
        "print(\"x train: \", x_train.shape)\n",
        "print(\"x test: \", x_test.shape)\n",
        "print(\"y train: \", y_train.shape)\n",
        "print(\"y test: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GIQFio6FceQ",
        "outputId": "22849443-aef9-496f-93cb-de4c97c17828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x train:  (30, 483)\n",
            "x test:  (30, 86)\n",
            "y train:  (483,)\n",
            "y test:  (86,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Why? Some ML algorithms work better with column vectors.\n",
        "so after tranpose, it becomes feature x sample\n"
      ],
      "metadata": {
        "id": "sQWr0kuqGc0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialise weights and bias\n",
        "def initialize_weights_and_bias(dimension):\n",
        "  w = np.full((dimension, 1),0.01)\n",
        "  b = 0.0\n",
        "  return (w,b)"
      ],
      "metadata": {
        "id": "ueZ0npqpGnPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- initialize_weights_and_bias function takes dimesnions as input. It must be an integer.\n",
        "- np.full creates a matrix. np.full((dimension, 1), 0.01): Creates an array filled with 0.01 values.\n",
        "- w must be a column matrix of value 0.01\n",
        "- b = 0 to avoid symmetry problems"
      ],
      "metadata": {
        "id": "RwYJAkiIHaEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define sigmoid activation\n",
        "def sigmoid(z):\n",
        "  y_head = 1/(1+np.exp(-z))\n",
        "  return y_head"
      ],
      "metadata": {
        "id": "6Fq8e7jmILYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sigmoid converts any number into a probability between 0 & 1.\n",
        "- Mathematically, g(z) and y_head represent the same thing, but in ML code, we use y_head because it represents predictions explicitly.\n",
        "- y_head = sigmoid(z) is the probability output (between 0 and 1).\n",
        "- y_head represents predictions, not just a function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rN0_m_wlJHex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#forward and backward prediction\n",
        "def forward_and_backward_prediction(w, b, x_train, y_train):\n",
        "  #forward prediction\n",
        "  z = np.dot(w.T, x_train) + b\n",
        "  y_head = sigmoid(z)\n",
        "  loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
        "  cost = (np.sum(loss))/x_train.shape[1]\n",
        "\n",
        "  #backward prediction\n",
        "  derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))/x_train.shape[1]\n",
        "  derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
        "  gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
        "  return cost, gradients"
      ],
      "metadata": {
        "id": "gOmD18AZMZr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(w, b, x_train, y_train, learning_rate, number_of_iterarion):\n",
        "    cost_list = []\n",
        "    cost_list2 = []\n",
        "    index = []\n",
        "\n",
        "    # updating(learning) parameters is number_of_iterarion times\n",
        "    for i in range(number_of_iterarion):\n",
        "        # make forward and backward propagation and find cost and gradients\n",
        "        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n",
        "        cost_list.append(cost)\n",
        "\n",
        "        # lets update\n",
        "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
        "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
        "        if i % 10 == 0:\n",
        "            cost_list2.append(cost)\n",
        "            index.append(i)\n",
        "            print (\"Cost after iteration % i: % f\" %(i, cost))\n",
        "\n",
        "    # update(learn) parameters weights and bias\n",
        "    parameters = {\"weight\": w, \"bias\": b}\n",
        "    plt.plot(index, cost_list2)\n",
        "    plt.xticks(index, rotation ='vertical')\n",
        "    plt.xlabel(\"Number of Iterarion\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.show()\n",
        "    return parameters, gradients, cost_list\n"
      ],
      "metadata": {
        "id": "eEazPATpNCfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, x_test):\n",
        "    # x_test is a input for forward propagation\n",
        "    z = sigmoid(np.dot(w.T, x_test)+b)\n",
        "    Y_prediction = np.zeros((1, x_test.shape[1]))\n",
        "\n",
        "    # if z is bigger than 0.5, our prediction is sign one (y_head = 1),\n",
        "    # if z is smaller than 0.5, our prediction is sign zero (y_head = 0),\n",
        "    for i in range(z.shape[1]):\n",
        "        if z[0, i]<= 0.5:\n",
        "            Y_prediction[0, i] = 0\n",
        "        else:\n",
        "            Y_prediction[0, i] = 1\n",
        "\n",
        "    return Y_prediction\n"
      ],
      "metadata": {
        "id": "0xWRdfBLTuT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_propagation(w, b, x_train, y_train):\n",
        "    \"\"\"\n",
        "    Compute the forward propagation and backpropagation for logistic regression.\n",
        "    \"\"\"\n",
        "    # Forward propagation\n",
        "    z = np.dot(w.T, x_train) + b\n",
        "    y_head = 1 / (1 + np.exp(-z))  # Sigmoid function\n",
        "\n",
        "    # Cost function\n",
        "    m = x_train.shape[1]\n",
        "    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n",
        "    cost = np.sum(loss) / m\n",
        "\n",
        "    # Backpropagation\n",
        "    dw = np.dot(x_train, (y_head - y_train).T) / m\n",
        "    db = np.sum(y_head - y_train) / m\n",
        "\n",
        "    gradients = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "    return cost, gradients\n"
      ],
      "metadata": {
        "id": "meTm3ZadTzNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n",
        "    \"\"\"\n",
        "    Perform gradient descent updates for logistic regression.\n",
        "    \"\"\"\n",
        "    cost_list = []\n",
        "\n",
        "    for i in range(number_of_iteration):\n",
        "        # Forward and backward propagation\n",
        "        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n",
        "        cost_list.append(cost)\n",
        "\n",
        "        # Update weights and bias\n",
        "        w = w - learning_rate * gradients[\"dw\"]\n",
        "        b = b - learning_rate * gradients[\"db\"]\n",
        "\n",
        "        # Print cost at every 10 iterations\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
        "\n",
        "    parameters = {\"weight\": w, \"bias\": b}\n",
        "\n",
        "    return parameters, gradients, cost_list\n"
      ],
      "metadata": {
        "id": "2mU3LYi5UIsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape(1, -1)  # Now (1, 30)\n",
        "y_test = y_test.reshape(1, -1)  # Now (1, 86)\n",
        "def initialize_weights_and_bias(dimension):\n",
        "    w = np.zeros((dimension, 1))  # Initialize weights as zeros\n",
        "    b = 0  # Initialize bias as zero\n",
        "    return w, b\n",
        "\n",
        "z = np.dot(w.T, x_train) + b\n",
        "y_head = 1 / (1 + np.exp(-z))  # Sigmoid function\n",
        "\n",
        "# Reshape to ensure correct broadcasting\n",
        "y_head = y_head.reshape(1, -1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ENKnQy3YVLp2",
        "outputId": "e2fda4ac-f795-421e-9456-b75b157e866e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'w' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-47bd41a63a33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_propagation(w, b, x_train, y_train):\n",
        "    \"\"\"\n",
        "    Compute forward propagation and backpropagation for logistic regression.\n",
        "    \"\"\"\n",
        "    # Forward propagation\n",
        "    z = np.dot(w.T, x_train) + b\n",
        "    y_head = 1 / (1 + np.exp(-z))  # Sigmoid function\n",
        "\n",
        "    # Reshape to ensure correct broadcasting\n",
        "    y_train = y_train.reshape(1, -1)\n",
        "    y_head = y_head.reshape(1, -1)\n",
        "\n",
        "    # Cost function (adding small epsilon to avoid log(0))\n",
        "    m = x_train.shape[1]\n",
        "    loss = -y_train * np.log(y_head + 1e-9) - (1 - y_train) * np.log(1 - y_head + 1e-9)\n",
        "    cost = np.sum(loss) / m\n",
        "\n",
        "    # Backpropagation\n",
        "    dw = np.dot(x_train, (y_head - y_train).T) / m\n",
        "    db = np.sum(y_head - y_train) / m\n",
        "\n",
        "    gradients = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "    return cost, gradients\n",
        "\n",
        "# Function to initialize weights\n",
        "def initialize_weights_and_bias(dimension):\n",
        "    w = np.zeros((dimension, 1))  # Initialize weights as zeros\n",
        "    b = 0  # Initialize bias as zero\n",
        "    return w, b\n",
        "\n",
        "# Logistic Regression Function\n",
        "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n",
        "    dimension = x_train.shape[0]  # Number of features\n",
        "    w, b = initialize_weights_and_bias(dimension)  # Initialize weights and bias\n",
        "\n",
        "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n",
        "\n",
        "    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n",
        "    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n",
        "\n",
        "    # Train / test accuracy\n",
        "    print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
        "    print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n"
      ],
      "metadata": {
        "id": "mzoiATMyWEGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_propagation(w, b, x_train, y_train):\n",
        "    \"\"\"\n",
        "    Compute forward propagation and backpropagation for logistic regression.\n",
        "    \"\"\"\n",
        "    # Forward propagation\n",
        "    z = np.dot(w.T, x_train) + b\n",
        "    y_head = 1 / (1 + np.exp(-z))  # Sigmoid function\n",
        "\n",
        "    # Reshape to ensure compatible shapes\n",
        "    y_train = y_train.reshape(1, -1)\n",
        "    y_head = y_head.reshape(1, -1)\n",
        "\n",
        "    # Cost function (adding small epsilon to avoid log(0))\n",
        "    m = x_train.shape[1]\n",
        "    loss = -y_train * np.log(y_head + 1e-9) - (1 - y_train) * np.log(1 - y_head + 1e-9)\n",
        "    cost = np.sum(loss) / m\n",
        "\n",
        "    # Backpropagation\n",
        "    dw = np.dot(x_train, (y_head - y_train).T) / m\n",
        "    db = np.sum(y_head - y_train) / m\n",
        "\n",
        "    gradients = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "    return cost, gradients\n",
        "\n"
      ],
      "metadata": {
        "id": "UlHbIYeZUrBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}